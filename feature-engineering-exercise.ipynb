{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.feature_selection\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    '''\n",
    "    take in a DataFrame and return train, validate, and test DataFrames.\n",
    "    return train, validate, test DataFrames.\n",
    "    '''\n",
    "    train_validate, test = train_test_split(df, test_size=.2, random_state=123)\n",
    "    train, validate = train_test_split(train_validate, \n",
    "                                       test_size=.3, \n",
    "                                       random_state=123)\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "df = data('tips')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the difference between\n",
    "# df.size, df[\"size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the size column because .size is a built-in Pandas attribute\n",
    "df = df.rename(columns={'size': 'number_of_people'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "# a. Create a column named tip_percentage. This should be the tip amount divided by the total bill.\n",
    "# b. Create a column named price_per_person. This should be the total bill divided by the party size.\n",
    "# c. Before using any of the methods discussed in the lesson, which features do you think would be most important for predicting the tip amount? The tip percentage?\n",
    "\n",
    "# Bracket notation to create new columns\n",
    "df[\"tip_percentage\"] = df.tip / df.total_bill\n",
    "df[\"price_per_person\"] = df.total_bill / df.number_of_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this specific exercise, we're only focusing on the numeric features\n",
    "df[\"dinner_time\"] = df.time == \"Dinner\"\n",
    "df = df[[\"total_bill\", \"tip\", \"number_of_people\", \"tip_percentage\", \"price_per_person\", \"dinner_time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train, validate, test = split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y splits\n",
    "target = \"tip\"\n",
    "\n",
    "X_train = train.drop(columns=[target])\n",
    "y_train = train[target]\n",
    "\n",
    "X_validate = validate.drop(columns=[target])\n",
    "y_validate = validate[target]\n",
    "\n",
    "X_test = test.drop(columns=[target])\n",
    "y_test = test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale (Make the thing)\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "# Fit the scaler, (fit the thing)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Use the scaler to transform train, validate, test (use the thing)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_validate_scaled = scaler.transform(X_validate)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1c\n",
    "# Before using any of the methods discussed in the lesson, which features do you think would be most important for predicting the tip amount? The tip percentage?\n",
    "# Perhaps tip percentage? NB. derived from target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1d\n",
    "# Use numeric features to predict tip_amount\n",
    "# Use select-K-best and RFE to select the top 2 features\n",
    "k = 2\n",
    "\n",
    "# Make the thing\n",
    "kbest = sklearn.feature_selection.SelectKBest(sklearn.feature_selection.f_regression, k=2)\n",
    "# fit the thing\n",
    "kbest.fit(X_train_scaled, y_train)\n",
    "# use the thing, \n",
    "# get_support() produces an array of booleans, so we can filter out the column names that matter the most\n",
    "X_train.columns[kbest.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do RFE\n",
    "\n",
    "# Make the thing(s)\n",
    "lm = sklearn.linear_model.LinearRegression()\n",
    "rfe = sklearn.feature_selection.RFE(lm, n_features_to_select=2)\n",
    "# Fit the thing\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "# use the thing\n",
    "rfe_columns = X_train.columns[rfe.support_].tolist()\n",
    "rfe_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "# Write a function named select_kbest that takes in the predictors (X), the target (y), and the number of features to select (k) and returns the names of the top k selected features based on the SelectKBest class. Test your function with the tips dataset. You should see the same results as when you did the process manually.\n",
    "\n",
    "def select_kbest(X, y, k):\n",
    "    # make the object\n",
    "    kbest = sklearn.feature_selection.SelectKBest(sklearn.feature_selection.f_regression, k=k)\n",
    "\n",
    "    # fit the object\n",
    "    kbest.fit(X, y)\n",
    "    \n",
    "    # use the object (.get_support() is that array of booleans to filter the list of column names)\n",
    "    return X.columns[kbest.get_support()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_kbest(X_train, y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3\n",
    "# Write a function named rfe that takes in the predictors, the target, and the number of features to select. It should return the top k features based on the RFE class. Test your function with the tips dataset. You should see the same results as when you did the process manually.\n",
    "\n",
    "# I use rfe as a variable name (perhaps more than I should), so we'll name this function select_rfe instead\n",
    "def select_rfe(X, y, k, return_rankings=False, model=LinearRegression()):\n",
    "    # Use the passed model, LinearRegression by default\n",
    "    rfe = sklearn.feature_selection.RFE(model, n_features_to_select=k)\n",
    "    rfe.fit(X, y)\n",
    "    features = X.columns[rfe.support_].tolist()\n",
    "    if return_rankings:\n",
    "        rankings = pd.Series(dict(zip(X.columns, rfe.ranking_)))\n",
    "        return features, rankings\n",
    "    else:\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_rfe(X_train, y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4\n",
    "# Load the swiss dataset and use all the other features to predict Fertility. Find the top 3 features using both select k best and recursive feature elimination (use the functions you just built to help you out).\n",
    "\n",
    "swiss = data('swiss')\n",
    "swiss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train, validate, test = split(swiss)\n",
    "\n",
    "# Setup X and y\n",
    "X_train = train.drop(columns='Fertility')\n",
    "y_train = train.Fertility\n",
    "\n",
    "X_validate = validate.drop(columns='Fertility')\n",
    "y_validate = validate.Fertility\n",
    "\n",
    "X_test = test.drop(columns='Fertility')\n",
    "y_test = test.Fertility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Use the scaler to transform train, validate, test\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_validate_scaled = scaler.transform(X_validate)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Turn everything into a dataframe\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_validate_scaled = pd.DataFrame(X_validate_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_train.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
